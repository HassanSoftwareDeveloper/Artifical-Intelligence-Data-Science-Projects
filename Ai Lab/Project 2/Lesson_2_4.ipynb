{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer learning and Cross validation**"
      ],
      "metadata": {
        "id": "bRoLrjEU29v-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QoCJr5Kt24L8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn.model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchinfo\n",
        "import torchvision\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch version : \", torch.__version__)\n",
        "print(\"torchvision version : \", torchvision.__version__)\n",
        "print(\"torchinfo version : \", torchinfo.__version__)\n",
        "print(\"numpy version : \", np.__version__)\n",
        "print(\"matplotlib version : \", matplotlib.__version__)\n",
        "\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaYu4e6V7GTV",
        "outputId": "38d262c4-77fc-4fe9-bbfb-94596a5e9503"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version :  2.6.0+cu124\n",
            "torchvision version :  0.21.0+cu124\n",
            "torchinfo version :  1.8.0\n",
            "numpy version :  2.0.2\n",
            "matplotlib version :  3.10.0\n",
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_qykECU7Lct",
        "outputId": "7d1ab418-5586-4baa-f20f-8e85bb2b2a4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                             **Loading Data**\n",
        "**We'll be working with the same undersampled dataset we created in an earlier lesson, in the data_p2/data_undersampled/train directory. We'll also be applying the same transformations. Let's load it. **"
      ],
      "metadata": {
        "id": "eW1R4Wdy7hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(\"data-p2\" , \" data_undersampled\" ,\"train\")\n",
        "\n",
        "print(\"Data directory:\", data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRac6vfP7nxV",
        "outputId": "f2e775a3-b555-4602-cb81-45adc6e8fc36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: data-p2/ data_undersampled/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertToRGB(object):\n",
        "    def __call__(self, img):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        return img"
      ],
      "metadata": {
        "id": "k101bt8q7wKd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.4.2: Create the set of transformations listed above. Use the means and standard deviations from the 022-explore-dataset lesson. **"
      ],
      "metadata": {
        "id": "YDScnXW570D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_normalized = transforms.Compose([\n",
        "    ConvertToRGB(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "transform_normalized"
      ],
      "metadata": {
        "id": "IBZwKAwg73F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.3: Use ImageFolder to read the files in our data_dir and apply our transformations.**"
      ],
      "metadata": {
        "id": "5ArL5XWG8BqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=data_dir,  transform=transform_normalized)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "Pzbj2uSiFVhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = dataset.classes\n",
        "classes"
      ],
      "metadata": {
        "id": "rP4tsV3hFafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "veZ6KfRR8DJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.4: Use the class_counts function on our dataset to verify all classes have the same number of observations.**"
      ],
      "metadata": {
        "id": "AQp0kp128HWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = class_counts(dataset)\n",
        "counts"
      ],
      "metadata": {
        "id": "cCN3_YJtFYLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 2.4.5: Create a DataLoader for the dataset. Use a batch size of ***\n",
        "."
      ],
      "metadata": {
        "id": "_aJyI6xt8JE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset_loader = DataLoader(dataset , batch_size=batch_size)\n",
        "\n",
        "print(f\"Batch shape: {next(iter(dataset_loader))[0].shape}\")"
      ],
      "metadata": {
        "id": "c967fvolFd0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Transfer Learning**\n",
        "We have our data, now we need a model. In the last lesson and the previous project, we built our own. But classifying images is a very common task, many people have already done it. Those people have already spent the time and computing resources to design and train a model. If we can get their architecture and weights, we can use theirs!\n",
        "\n",
        "Thankfully, many models like this are publicly available. These are called pre-trained models. PyTorch comes with some included. Here we'll load a model called resnet."
      ],
      "metadata": {
        "id": "bZI5dbxvFHxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)"
      ],
      "metadata": {
        "id": "DQ2VC0nsFmoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.4.6: Get the shape of the test_batch of data, and use that as the input_size when you call summary on the model. ** **bold text**"
      ],
      "metadata": {
        "id": "pCjZSUIu8Vn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = next(iter(dataset_loader))[0]\n",
        "batch_shape = test_batch.shape\n",
        "\n",
        "# Create the model summary\n",
        "summary(model,input_size=batch_shape)"
      ],
      "metadata": {
        "id": "8nPimSjpFjgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has many layers! We don't need to worry about the details, since we won't be building or training it. In fact, we need to mark the layers to tell them not to train.\n",
        "\n",
        "All models come with a parameters method that gives us access to the model's weights. We can loop through this to set requires_grad = False on all of the weights. This tells the system not to take their derivatives, so backpropagation doesn't update them."
      ],
      "metadata": {
        "id": "00d9Cai-Hk6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for params in model.parameters():\n",
        "    params.requires_grad = False"
      ],
      "metadata": {
        "id": "TQuez3fzHuji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model was trained for a different purpose than we need it for. We can see this by looking at the shape of the output. But our model is very large, so before we run it we'll want to make sure both the model and the test_batch are on the GPU."
      ],
      "metadata": {
        "id": "l9lWftSbH5hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.7: Move the model and the test_batch to our device.**"
      ],
      "metadata": {
        "id": "bmuKFg838bh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to device\n",
        "model.to(device)\n",
        "\n",
        "# Move our test_batch to device\n",
        "test_batch_cuda = test_batch.to(device)\n",
        "\n",
        "print(\"Test batch is running on:\", test_batch_cuda.device)"
      ],
      "metadata": {
        "id": "KO1c7PHKHZWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.8: Run the model on the test batch, and check the shape of the output.**"
      ],
      "metadata": {
        "id": "13gG7nF18fXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_test_out = model(test_batch_cuda)\n",
        "model_test_shape = model_test_out.shape\n",
        "\n",
        "print(\"Output shape:\", model_test_shape)"
      ],
      "metadata": {
        "id": "yZW5QZKWIKby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying the Network to Our Task\n",
        "This model was meant for a task with 1000\n",
        " classes. We only have 5\n",
        ", so that's not going to work for us. Even if they were the same number of classes, it wouldn't work, since it was trained for a different task.\n",
        "\n",
        "But we can replace the final layer with our own network. The rest of the network will still do the image processing, and provide our layer with good inputs. Our network will do the final classification. This process of using most of an already trained model is called transfer learning.\n",
        "\n",
        "Which layer is the last one? We can access the list of layers with the named_modules method. It returns a generator, which we can convert to a list to get the last element."
      ],
      "metadata": {
        "id": "QIsvZJO9IMQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.named_modules())[-1]"
      ],
      "metadata": {
        "id": "Xe_lg3MbJMUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks right — it's a linear layer with\n",
        " neurons (and hence outputs). The thing we really wanted to know was its name — fc. Now we can access it with model.fc. We'll need to know how many inputs it takes to be able to replace it. It's a Linear layer, so the number of inputs it takes is recorded in the in_features attribute."
      ],
      "metadata": {
        "id": "oXsFpmpVJ5_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.9:: Get the number of input features into the last layer by using in_features attribute. Save that to the in_features variable.**"
      ],
      "metadata": {
        "id": "fDq_n12v8M4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = model.fc.in_features\n",
        "in_features"
      ],
      "metadata": {
        "id": "yOMyL9N6JJTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a network to replace it. It will need to take the same inputs, but produce our outputs.\n",
        "\n",
        "We'll make a small network to do our classification. As before, we'll build it with the Sequential container."
      ],
      "metadata": {
        "id": "8F50MMQAKBVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = torch.nn.Sequential()"
      ],
      "metadata": {
        "id": "sYfaCN3nKEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build up a network with the following structure\n",
        "\n",
        "Linear layer of 256 neurons\n",
        "ReLU\n",
        "Dropout\n",
        "Linear layer of 5 neurons for output"
      ],
      "metadata": {
        "id": "CL1mpZUMKGzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.10: Make a Linear layer that takes the same inputs as the fc layer and produces**\n",
        "\n",
        "** outputs. Add it to our classifier network.**"
      ],
      "metadata": {
        "id": "EZZA-kT38y_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_layer = torch.nn.Linear(in_features=in_features , out_features=256)\n",
        "\n",
        "\n",
        "# Add the layer to our classifier\n",
        "classifier.append(classification_layer)"
      ],
      "metadata": {
        "id": "IPtDYGEHJ-9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's add the ReLU and the Dropout.\n",
        "\n",
        "We'll use the default settings for Dropout, which offer a good balance between speed and preventing overfitting."
      ],
      "metadata": {
        "id": "7mwfhUMfMMky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.append(torch.nn.ReLU())\n",
        "classifier.append(torch.nn.Dropout())"
      ],
      "metadata": {
        "id": "RB5mGNJkMK1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can finish off our classifier with an output layer that produces one output for each of our classes."
      ],
      "metadata": {
        "id": "VaJMGBNrMQFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.4.11: Make a Linear layer that takes the previous layer as input and produces **\n",
        "** outputs. Add it to our classifier network.**"
      ],
      "metadata": {
        "id": "9-O_HqE088Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = torch.nn.Linear(in_features=256,out_features=5)\n",
        "\n",
        "# Add the layer to our classifier\n",
        "classifier.append(output_layer)"
      ],
      "metadata": {
        "id": "-f2uyRc8NLq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we want to do two things: remove the output layer in ResNet that's wrong for us, and add our classifier. We can do both at the same time by replacing fc with our classifier network."
      ],
      "metadata": {
        "id": "p-qpnfnnNOwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Task 2.4.12: Call summary on the model again. You can use the same batch_shape we used earlier.**"
      ],
      "metadata": {
        "id": "5iC9A3sC9IcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, at this point we'd split our data into training and validation sets. We'd train on the training set, and check that the model is performing well on the validation set. This lets us see how the model does on data it wasn't trained on and detect overfitting. Instead, we'll use k-fold cross-validation, which splits our data into\n",
        " \"folds\".\n",
        "\n",
        "PyTorch doesn't have a dedicated tool for this. Instead, we'll use the KFold splitting tool from scikit-learn. It needs to know how many splits to make and if we want to shuffle the order of observations. Since our data is ordered (we get all of one class, then all of the next class, and so on), we do want to shuffle. We have been using 20% of our data for validation so far, so let's keep doing that."
      ],
      "metadata": {
        "id": "oYvli8ntOhpL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfvVQcASOgX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Task 2.4.13:** **Set $k$ to get each fold to be 20% of the data. How many parts should we break the data into to get that? **"
      ],
      "metadata": {
        "id": "hdZA-oIi9Tnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "\n",
        "kfold_splitter = sklearn.model_selection.KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "train_nums, val_nums = next(kfold_splitter.split(range(100)))\n",
        "fold_fraction = len(val_nums) / (len(train_nums) + len(val_nums))\n",
        "print(f\"One fold is {100*fold_fraction:.2f}%\")"
      ],
      "metadata": {
        "id": "xEFnDs0KO2Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to adjust our training somewhat to use cross-validation. We won't be using a fixed training and validation set. Instead, we'll get a training set of 80% of the data, and a validation set of 20% of the data from our splitter. We'll train with this, then reset our model and get the next training and validation sets and repeat.\n",
        "\n",
        "We'll still be able to reuse most of our code from before. Let's import it from the training.py file."
      ],
      "metadata": {
        "id": "TR-NAv3zO7mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import predict, train"
      ],
      "metadata": {
        "id": "3JDpc9mRO8HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OWYQ7mPO-Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.14: Define cross-entropy as the loss function and set the Adam optimizer to be the optimizer. You can use the default learning settings.**"
      ],
      "metadata": {
        "id": "tEXLzupu902W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "NHslBlCEO3B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To get an accurate measure, we'll need to reset the model when we change which fold is the validation set. Otherwise the model will have seen that data, which means it's not a good validation set!\n",
        "\n",
        "We only want to reset the part we added, since we're not training the rest of the model. We can do that using reset_parameters on just the layers that we added.\n",
        "\n",
        "Sequential named the layers for us, as \"0\", \"1\", etc."
      ],
      "metadata": {
        "id": "moTjO3DmPVla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc"
      ],
      "metadata": {
        "id": "cKoPZVT7Pgrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_classifier(model):\n",
        "    model.fc.get_submodule(\"0\").reset_parameters()\n",
        "    model.fc.get_submodule(\"3\").reset_parameters()"
      ],
      "metadata": {
        "id": "bq5RJmiXPhSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have one more thing to set before we can start training. We'll need to decide how many epoch to train our model. Through some testing, we found the model stops improving by the\n",
        " epoch mark. To prevent this from becoming overfitting, we'll do a form of early stopping and only train for\n",
        " epochs."
      ],
      "metadata": {
        "id": "eeCaVq7jPlaj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cBGa1v2XPlIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.15: Set num_epochs so the model only trains for\n",
        " epochs.**"
      ],
      "metadata": {
        "id": "AHvvOX4B93zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 6"
      ],
      "metadata": {
        "id": "l_IpN8mrP1EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're ready. For k-fold, we'll train in a loop that will run\n",
        " times. In each run, we'll have one fold as our validation set and the rest as training.\n",
        "\n",
        "On each loop we'll do a few things:\n",
        "\n",
        "Get which observations are in the training set and which are in the validation set from our k-fold splitter.\n",
        "Create a training and a validation data loader\n",
        "Reset the classifier part of our model\n",
        "Train the model with this training set and validation set\n",
        "Record the losses and accuracies from the training process\n",
        "We're setting an option on the train function we haven't used before: use_train_accuracy=False. We won't get the accuracy on the training data, but it will make the training process faster.\n",
        "\n",
        "This next cell runs the training process. It can take quite a while."
      ],
      "metadata": {
        "id": "Rgo_ujoLP4Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can safely skip this cell and load the model in the next cell\n",
        "\n",
        "training_records = {}\n",
        "fold_count = 0\n",
        "\n",
        "for train_idx, val_idx in kfold_splitter.split(np.arange(len(dataset))):\n",
        "    fold_count += 1\n",
        "    print(\"*****Fold {}*****\".format(fold_count))\n",
        "\n",
        "    # Make train and validation data loaders\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Reset the model\n",
        "    reset_classifier(model)\n",
        "\n",
        "    # Train\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = train(\n",
        "        model,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        use_train_accuracy=False,\n",
        "    )\n",
        "\n",
        "    # Save training results for graphing\n",
        "    training_records[fold_count] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"val_accuracies\": val_accuracies,\n",
        "    }\n",
        "\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "U2JIdr1CP0E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"pretrained_model.pth\", weights_only=False)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "cWi_VDnNQBNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've also made available the training_records resulting from the pretrained model:"
      ],
      "metadata": {
        "id": "i8bRL-A1QE3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('training_records.pkl', 'rb') as fp:\n",
        "    training_records = pickle.load(fp)"
      ],
      "metadata": {
        "id": "O-6qlM2mQC9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Performance**\n",
        "We gathered a lot of information as we were training. For each fold, we have the losses and accuracy at each training epoch. Let's visualize these. They were stored in a dictionary with a key for each fold, just numbered\n",
        " to\n",
        "."
      ],
      "metadata": {
        "id": "dF9snIES5QjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Performance**\n",
        "We gathered a lot of information as we were training. For each fold, we have the losses and accuracy at each training epoch. Let's visualize these. They were stored in a dictionary with a key for each fold, just numbered\n",
        " to\n",
        "."
      ],
      "metadata": {
        "id": "V35ffqfVQHU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(training_records))\n",
        "training_records.keys()"
      ],
      "metadata": {
        "id": "7-vF2fit6ATw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(training_records[1]))\n",
        "training_records[1].keys()"
      ],
      "metadata": {
        "id": "-XHAtIFH6Cvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_records[1][\"train_losses\"]"
      ],
      "metadata": {
        "id": "Q58vMz3S6EbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_folds(data, measurement):\n",
        "    for fold in data.keys():\n",
        "        plt.plot(data[fold][measurement], label=f\"Fold {fold}, {measurement}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "9KZvitMj6G7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_all_folds(training_records, \"train_losses\")"
      ],
      "metadata": {
        "id": "d_uTFELu6J6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.16: Use the plot_all_folds function to look at the validation losses (val_losses).**"
      ],
      "metadata": {
        "id": "cPAGBYy7-Lkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation losses\n",
        "plot_all_folds(training_records,\"val_losses\")"
      ],
      "metadata": {
        "id": "CA4BsW1i6NCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.17: Use the plot_all_folds function to look at the validation accuracy (val_accuracies).**"
      ],
      "metadata": {
        "id": "s8kUdAEQ-QQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation accuracies\n",
        "plot_all_folds(training_records,\"val_accuracies\")"
      ],
      "metadata": {
        "id": "z5qgGxNa62I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There's more variation between the folds in the validation metrics, but they're all fairly flat or getting a little worse at the end. This suggests our model is full trained, and we may be getting close to overfitting.\n",
        "\n",
        "Usually the metrics that we actually look at are not the individual folds. Instead, we average the measurements from each fold together, for the last epoch."
      ],
      "metadata": {
        "id": "6QfetR0165zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fold_average(data, measurement):\n",
        "    return np.mean([data[fold][measurement][-1] for fold in data])\n",
        "\n",
        "\n",
        "for measurement in training_records[1].keys():\n",
        "    avg_measure = fold_average(training_records, measurement)\n",
        "    print(f\"Averaged {measurement}: {avg_measure}\")"
      ],
      "metadata": {
        "id": "0LQvPp1S637v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also make the confusion matrix to see how it did, and if specific classes are the issue. Usually we'd make the confusion matrix on a validation set. But we didn't do a normal train-validation split here.\n",
        "\n",
        "To get around this, we'll note how the k-fold worked. It created a training and validation split at each step, and trained the model. So right now, the model has been trained on the training set for the last fold.\n",
        "\n",
        "The val_loader variable should still be the validation set for that last fold. Let's use that. We can use the predict function from our training.py as we ha"
      ],
      "metadata": {
        "id": "aeH-7Jb_7OHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.18: Calculate the predictions for val_loader. Remember to set the device.**"
      ],
      "metadata": {
        "id": "sbkT0qkb-Tsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **AI Lab: Deep Learning for Computer Vision**\n",
        "# **WorldQuant University**\n",
        "#\n",
        "#\n",
        "\n",
        "# **Usage Guidelines**\n",
        "#\n",
        "# This file is licensed under Creative Commons Attribution-NonCommercial-\n",
        "# NoDerivatives 4.0 International.\n",
        "#\n",
        "# You **can** :\n",
        "#\n",
        "#   * ✓ Download this file\n",
        "#   * ✓ Post this file in public repositories\n",
        "#\n",
        "# You **must always** :\n",
        "#\n",
        "#   * ✓ Give credit to WorldQuant University for the creation of this file\n",
        "#   * ✓ Provide a link to the license\n",
        "#\n",
        "# You **cannot** :\n",
        "#\n",
        "#   * ✗ Create derivatives or adaptations of this file\n",
        "#   * ✗ Use this file for commercial purposes\n",
        "#\n",
        "# Failure to follow these guidelines is a violation of your terms of service and\n",
        "# could lead to your expulsion from WorldQuant University and the revocation\n",
        "# your certificate.\n",
        "#\n",
        "#\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def class_counts(dataset):\n",
        "    c = Counter(x[1] for x in tqdm(dataset))\n",
        "    try:\n",
        "        class_to_index = dataset.class_to_idx\n",
        "    except AttributeError:\n",
        "        class_to_index = dataset.dataset.class_to_idx\n",
        "    return pd.Series({cat: c[idx] for cat, idx in class_to_index.items()})\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, loss_fn, data_loader, device=\"cpu\"):\n",
        "    training_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over all batches in the training set to complete one epoch\n",
        "    for inputs, targets in tqdm(data_loader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "    return training_loss / len(data_loader.dataset)\n",
        "\n",
        "\n",
        "def predict(model, data_loader, device=\"cpu\"):\n",
        "    all_probs = torch.tensor([]).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(data_loader, desc=\"Predicting\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            probs = torch.nn.functional.softmax(output, dim=1)\n",
        "            all_probs = torch.cat((all_probs, probs), dim=0)\n",
        "\n",
        "    return all_probs\n",
        "\n",
        "\n",
        "def score(model, data_loader, loss_fn, device=\"cpu\"):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(data_loader, desc=\"Scoring\", leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output, targets)\n",
        "            total_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "            correct = torch.eq(torch.argmax(output, dim=1), targets)\n",
        "            total_correct += torch.sum(correct).item()\n",
        "\n",
        "    n_observations = data_loader.batch_size * len(data_loader)\n",
        "    average_loss = total_loss / n_observations\n",
        "    accuracy = total_correct / n_observations\n",
        "    return average_loss, accuracy\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=20,\n",
        "    device=\"cpu\",\n",
        "    use_train_accuracy=True,\n",
        "):\n",
        "    # Track the model progress over epochs\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Train one epoch\n",
        "        training_loss = train_epoch(model, optimizer, loss_fn, train_loader, device)\n",
        "\n",
        "        # Evaluate training results\n",
        "        if use_train_accuracy:\n",
        "            train_loss, train_accuracy = score(model, train_loader, loss_fn, device)\n",
        "        else:\n",
        "            train_loss = training_loss\n",
        "            train_accuracy = 0\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Test on validation set\n",
        "        validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
        "        val_losses.append(validation_loss)\n",
        "        val_accuracies.append(validation_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"    Training loss: {train_loss:.2f}\")\n",
        "        if use_train_accuracy:\n",
        "            print(f\"    Training accuracy: {train_accuracy:.2f}\")\n",
        "        print(f\"    Validation loss: {validation_loss:.2f}\")\n",
        "        print(f\"    Validation accuracy: {validation_accuracy:.2f}\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "\n",
        "# This file © 2024 by WorldQuant University is licensed under CC BY-NC-ND 4.0.\n"
      ],
      "metadata": {
        "id": "NjAf92g08Cn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you loaded the pre-trained model, make sure you have the correct validation set\n",
        "_, val_idx = list(kfold_splitter.split(np.arange(len(dataset))))[-1]\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Xf2f-aNy7PYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.19: Make the same confusion matrix we made in earlier lessons. You'll need to either move the predictions to cpu or convert them to a list. The labels will be our classes.**"
      ],
      "metadata": {
        "id": "DVQDbtPJ-Wj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets = []\n",
        "\n",
        "for _, labels in tqdm(val_loader):\n",
        "    targets.extend(labels.tolist())"
      ],
      "metadata": {
        "id": "-uiwm6WM9Hkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(targets_val.cpu(), predictions_val.cpu())\n",
        "\n",
        "\n",
        "disp =ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "plt.figure(figsize=(10,8))\n",
        "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\")\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "FHgrsWaN8wnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"model/resnet_k_folds\")"
      ],
      "metadata": {
        "id": "Lce7MD9E9Ctw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "Good work! We have gained one of the most important abilities — we can now take advantage of the hard work of professionals. Experts put a lot of time and computing resources into training models, and transfer learning lets us use them for our own purposes. Here are the key takeaways:\n",
        "\n",
        "Many well trained networks are available publicly\n",
        "We can download them and modify the last layer to suit our specific task\n",
        "We only need to train the part we added\n",
        "Cross validation is an alternative way to measure how well our model performs\n",
        "In the next lesson, we'll do a bit more transfer learning. We'll also look at callbacks, a powerful way of modifying the training process to better meet our needs."
      ],
      "metadata": {
        "id": "MjxA4TUt8_0g"
      }
    }
  ]
}