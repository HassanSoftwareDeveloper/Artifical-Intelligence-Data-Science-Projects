{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_Lk1ub_FIbRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jktx1gAGLvZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchinfo\n",
        "import torchvision\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "id": "N_O7i_ziIjt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll work with images of crop disease from Uganda which we prepared in the previous lesson. You may remember that we created an undersampled dataset that has a uniform distribution across classes. Let's use that dataset.\n",
        "\n",
        "The data is in the data_p2 folder within which is the data_undersampled folder. In that folder we have the train folder that contains the training data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Task 2.3.1: Assign train_dir the path to the training data. Follow the pattern of data_dir.**"
      ],
      "metadata": {
        "id": "9SmpZWKmIpd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(\"data_p2\", \"data_undersampled\")\n",
        "train_dir = os.path.join(data_dir,\"train\")\n",
        "\n",
        "print(\"Data Directory:\", data_dir)\n",
        "print(\"Training Data Directory:\", train_dir)"
      ],
      "metadata": {
        "id": "HL8wN2D4ImaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.2: Create a list of class names using os.listdir.**\n",
        "\n"
      ],
      "metadata": {
        "id": "SXtSJlfFJR9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(train_dir)\n",
        "\n",
        "print(\"List of classes:\", classes)"
      ],
      "metadata": {
        "id": "ag94EKL5JNnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following what we did in the previous lesson to standardize the images, we'll again use the same set of transformations:\n",
        "\n",
        "Convert any grayscale images to RGB format with a custom class\n",
        "Resize the image, so that they're all the same size (we chose\n",
        " x\n",
        ")\n",
        "Convert the image to a Tensor of pixel values\n",
        "Normalize the data (we normalize each color channel separately)\n",
        "Here's the custom transformation that we've used before which converts images to RGB format:"
      ],
      "metadata": {
        "id": "SepprMrKJXh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertToRGB(object):\n",
        "    def __call__(self, img):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        return img"
      ],
      "metadata": {
        "id": "QoeXMK5lJYTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.3.3: Complete the transformation pipeline below. It's missing the last two steps (converting images to PyTorch tensors and normalizing them). In the normalization step, make sure to use the mean and std values from the previous lesson.**"
      ],
      "metadata": {
        "id": "9JE80UPOKo6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformation to apply to the images\n",
        "transform_normalized = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        # Convert images to tensors\n",
        "        # ...\n",
        "        # Normalize the tensors (copy the mean and std from previous lesson!)\n",
        "        transforms.ToTensor() ,\n",
        "        transforms.Normalize(\n",
        "    mean=[0.4326, 0.4953, 0.3120],\n",
        "    std=[0.2178, 0.2214, 0.2091]\n",
        ")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(type(transform_normalized))\n",
        "print(\"--------------\")\n",
        "print(transform_normalized)"
      ],
      "metadata": {
        "id": "-qOTC3mhKK2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zJJEAN5qKqjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.4: Make a normalized dataset using ImageFolder from datasets and print the length of the dataset.**"
      ],
      "metadata": {
        "id": "zYmlPwykgNbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir,transform=transform_normalized)\n",
        "\n",
        "print('Length of dataset:', len(dataset))"
      ],
      "metadata": {
        "id": "WdO8K4roKqqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Train and validation splitting**\n",
        "We'll follow good practice and divide our data into two parts. One part will be the data we'll train our model on. The second part will be used to evaluate the model on images it hasn't seen in training.\n",
        "\n",
        "This is an important step in order for us to check how good the model is. If it makes good predictions on the training data but not on the validation data, we'll know the model's overfit.\n",
        "\n",
        "**Task 2.3.5: Use random_split to create a 80/20 split (training dataset should have 80% of the data, validation dataset should have 20% of the data).**"
      ],
      "metadata": {
        "id": "qpfpUahrgRFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important, don't change this!\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset,[0.8,0.2], generator=g)\n",
        "\n",
        "print(\"Length of training dataset:\", len(train_dataset))\n",
        "print(\"Length of validation dataset:\", len(val_dataset))"
      ],
      "metadata": {
        "id": "9T1NVdGsgU4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.6: Compute the length of the entire dataset, the training dataset and the validation dataset. We've added the code that computes the percentage of data that's training data and percentage that's validation.**"
      ],
      "metadata": {
        "id": "g8ac-hiagXJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length_dataset = len(dataset)\n",
        "length_train = len(train_dataset)\n",
        "length_val = len(val_dataset)\n",
        "\n",
        "percent_train = np.round(100 * length_train / length_dataset, 2)\n",
        "percent_val = np.round(100 * length_val / length_dataset, 2)\n",
        "\n",
        "print(f\"Train data is {percent_train}% of full data\")\n",
        "print(f\"Validation data is {percent_val}% of full data\")"
      ],
      "metadata": {
        "id": "8P1VBhD1gZYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.7: Use class_counts function on the entire dataset and visualize the results with a bar chart. Note that computing dataset_counts may take a long time.**"
      ],
      "metadata": {
        "id": "NSblpIXzgbDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import class_counts\n",
        "\n",
        "dataset_counts = class_counts(dataset)\n",
        "dataset_counts.sort_values().plot(kind=\"bar\")\n",
        "\n",
        "# Make a bar chart from the function output\n",
        "\n",
        "# Add axis labels and title\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency [count]\")\n",
        "plt.title(\"Distribution of Classes in Entire Dataset\");"
      ],
      "metadata": {
        "id": "vRFkr22HgcZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0zBi34TggnO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Task 2.3.8: Use the class_counts function and pandas plotting to make the same plot for the training data."
      ],
      "metadata": {
        "id": "ejAbj0q8ggib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_counts = class_counts(train_dataset)\n",
        "# Make a bar chart from the function output\n",
        "train_counts.sort_values().plot(kind=\"bar\")\n",
        "\n",
        "# Add axis labels and title\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency [count]\")\n",
        "plt.title(\"Distribution of Classes in Training Dataset\");"
      ],
      "metadata": {
        "id": "rj58u4v7ggBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.9: Use the class_counts function and pandas plotting to get the breakdown across classes for the validation split.**"
      ],
      "metadata": {
        "id": "he9IMDLIgk_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_counts = class_counts(val_dataset)\n",
        "\n",
        "# Make a bar chart from the function output\n",
        "val_counts.sort_values().plot(kind=\"bar\")\n",
        "# Add axis labels and title\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency [count]\")\n",
        "plt.title(\"Distribution of Classes in Validation Dataset\");"
      ],
      "metadata": {
        "id": "unkjjcItgiT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.10: Create the training loader. Make sure to set shuffling to be on.**"
      ],
      "metadata": {
        "id": "RUTMQKnCgqhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "print(type(train_loader))"
      ],
      "metadata": {
        "id": "RTNfgLF8gtay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.11: Create the validation loader. Make sure to set shuffling to be off.**"
      ],
      "metadata": {
        "id": "Gpuzu4RjgynS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "print(type(val_loader))"
      ],
      "metadata": {
        "id": "80Sda_63g2PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Task 2.3.12: Print the shape of a batch of images and the shape of a batch of labels. **bold text**"
      ],
      "metadata": {
        "id": "2jCxYGU5g4VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "image_shape = images.shape\n",
        "print(\"Shape of batch of images\", image_shape)\n",
        "\n",
        "label_shape = labels.shape\n",
        "print(\"Shape of batch of labels:\", label_shape)"
      ],
      "metadata": {
        "id": "0_zYa2apg5fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Convolutional Neural Network**\n",
        "As we learned in the previous project, a network architecture suitable for image classification is the convolutional neural network (CNN). It primarily consists of a sequence of convolutional and max pooling layers. These layers are followed by some fully connected layers and an output layer.\n",
        "\n",
        "Let's build a CNN!\n",
        "\n",
        "Same as previously, we'll use the nn.Sequential class from PyTorch to define the architecture. We'll start with an empty model and append layers to it one by one."
      ],
      "metadata": {
        "id": "jAR8dJ3ig9Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential()"
      ],
      "metadata": {
        "id": "lF5Iyl_0g-mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2.3.13: Define the first convolutional layer of our network. Remember that we have three color channels, so set in_channels=3. Use\n",
        " kernels, each of siz\n",
        " and set padding to\n"
      ],
      "metadata": {
        "id": "Wn1--9J8hCXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional layer 1 (sees 3x224x224 image tensor)\n",
        "conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1)\n",
        "model.append(conv1)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "YjSHcE5NhAfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_pool1 = nn.MaxPool2d(2, 2)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool1)"
      ],
      "metadata": {
        "id": "e-rnlsPKhNdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.14: Define another convolutional layer of our network. This one should hav**e\n"
      ],
      "metadata": {
        "id": "i-9eZeCUhKSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional layer 2 (sees 16x112x112 tensor)\n",
        "conv2 = nn.Conv2d(16,32,3,padding=1)\n",
        "max_pool2 = nn.MaxPool2d(2, 2)\n",
        "model.append(conv2)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool2)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "PI7_TFzkhUHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 2.3.15: Define the last convolutional layer of our network. This one should have 64\n",
        " kernels 3. Again use kernels of size\n",
        " and padding of 1\n"
      ],
      "metadata": {
        "id": "wPs3RH45hrUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional layer 3 (sees 32x56x56 tensor)\n",
        "conv3 = nn.Conv2d(32,64,3,padding=1)\n",
        "max_pool3 = nn.MaxPool2d(2, 2)\n",
        "model.append(conv3)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool3)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "WDl6O9HLh2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.append(torch.nn.Flatten())\n",
        "model.append(nn.Dropout(0.5))"
      ],
      "metadata": {
        "id": "wk4uznfch6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.16: Add a Linear layer to the model. You'll need to tell it the size of the input, and how many neurons we want in the layer (let's use\n",
        " 500 neurons)**"
      ],
      "metadata": {
        "id": "Noopyg0Vh8hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear layer (64 * 28 * 2**8 -> 500)\n",
        "linear1 = torch.nn.Linear(64*28*28,500)\n",
        "model.append(linear1)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "uignZAZriBQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Task 2.3.17: Add the output layer to the model.**"
      ],
      "metadata": {
        "id": "1gYoxsXWiR6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear layer (500 -> 5)\n",
        "output_layer = nn.Linear(500,5)\n",
        "model.append(output_layer)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "t1K_CoiBiPWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.18: Define cross-entropy as the loss function and set Adam optimizer to be the optimizer. You can use the default learning rate lr=0.001.**"
      ],
      "metadata": {
        "id": "2-z9BgLGiXMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "print(loss_fn)\n",
        "print(\"----------------------\")\n",
        "print(optimizer)"
      ],
      "metadata": {
        "id": "Mq730hMTiYQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "7b0tl_qaibPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "height = 224\n",
        "width = 224\n",
        "summary(model, input_size=(batch_size, 3, height, width))"
      ],
      "metadata": {
        "id": "3MWPJ71gidCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from training import train"
      ],
      "metadata": {
        "id": "7SQr0JWsie_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.19: Use the train function to train the model for 15 epochs. Note that this may take a long time to run.**"
      ],
      "metadata": {
        "id": "9N2jHMqOihAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OqoqYzqair_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"model_trained.pth\", weights_only=False)\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "dnerq37-ig1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('post_train_evaluation_metrics.csv')"
      ],
      "metadata": {
        "id": "Dgnj-s6XjkwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = df['Train Loss']\n",
        "valid_losses = df['Validation Loss']\n",
        "\n",
        "train_accuracies = df['Train Accuracy']\n",
        "valid_accuracies = df['Validation Accuracy']"
      ],
      "metadata": {
        "id": "PrEKH5S8jnW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(valid_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(valid_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Accuracy over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53m3bpcljpGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh no, we’re overfitting! 🤯\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, capturing noise and details to the extent that it negatively impacts the model’s performance on new data. The symptoms of overfitting in our training results are:\n",
        "\n",
        "High training accuracy: The model performs exceptionally well on the training data.\n",
        "Increasing validation loss: Despite improvements in training loss and accuracy, the validation loss starts to increase after reaching a certain point.\n",
        "Stagnant or decreasing validation accuracy: The model’s ability to generalize to unseen data does not improve or worsens as training progresses.\n",
        "Addressing Overfitting\n",
        "\n",
        "To mitigate overfitting, consider the following strategies:\n",
        "\n",
        "Data Augmentation: Augment the training data by applying random transformations (e.g., rotations, scaling, flips) to generate new training samples. This can help the model generalize better.\n",
        "\n",
        "Dropout: Introduce dropout layers into our network. Dropout randomly sets a fraction of input units to 0 during training, which helps prevent the model from becoming too reliant on any single feature.\n",
        "\n",
        "Regularization: Apply regularization techniques, such as L1 or L2 regularization, which add a penalty on the magnitude of network parameters. This can discourage complex models that overfit.\n",
        "\n",
        "Early Stopping: Monitor the model’s performance on a validation set and stop training when the validation loss starts to increase, which is a sign that the model's beginning to overfit.\n",
        "\n",
        "Reduce Model Complexity: Simplify your model by reducing the number of layers or the number of units in the layers. A simpler model may generalize better.\n",
        "\n",
        "Use More Data: If possible, adding more data can help the model learn better and generalize well to new, unseen data.\n",
        "\n",
        "Batch Normalization: Although primarily used to help with training stability and convergence, batch normalization can sometimes also help with overfitting by regularizing the model somewhat.\n",
        "\n",
        "Cross-validation: Helps prevent overfitting by testing the model on different parts of the data, ensuring it performs well on new data."
      ],
      "metadata": {
        "id": "ofAvFXU0j24r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3.20: Use the predict function from training.py to compute probabilities that our model predicts on the validation data. The rest of the code provided will take these probabilities and compute the predicted classes.**"
      ],
      "metadata": {
        "id": "Sl8wIWYAj5Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import predict\n",
        "\n",
        "probabilities_val = predict(model,val_loader,device)\n",
        "predictions_val = torch.argmax(probabilities_val, dim=1)\n",
        "\n",
        "print(predictions_val)"
      ],
      "metadata": {
        "id": "wB5vTRWBj7kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_val = torch.cat(\n",
        "    [labels for _, labels in tqdm(val_loader, desc=\"Get Labels\")]\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "9ak-eI91j9Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(targets_val.cpu(), predictions_val.cpu())\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95KTpTp0j-rM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}