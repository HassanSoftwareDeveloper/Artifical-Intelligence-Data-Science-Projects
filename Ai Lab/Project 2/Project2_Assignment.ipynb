{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vD52JG8XOeYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcnHIlFKOeuN",
        "outputId": "8edf0d07-6291-475e-905d-1a33495c1f87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhV_varmNRlU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# Important! Don't change this\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be making large networks, so we'll get the GPU if it's available."
      ],
      "metadata": {
        "id": "EA9uCbcnN1lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "id": "sdxuLRDgNhbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Explore the Data**"
      ],
      "metadata": {
        "id": "LwcGMdGxN4hF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!ls -l potato_dataset/"
      ],
      "metadata": {
        "id": "8XdxoH3kORe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l potato_dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z440QyIpOS9m",
        "outputId": "b1abaa7b-8611-46d5-af08-f00919861c43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'potato_dataset/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.1: Make a list of the names of the classes we'll be working with.**"
      ],
      "metadata": {
        "id": "xmUu30sLPqu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join('potato_dataset','train')\n",
        "classes = os.listdir(train_dir)\n",
        "\n",
        "print(classes)\n",
        "print(f\"Number of classes: {len(classes)}\")"
      ],
      "metadata": {
        "id": "8b4hC2d6PnCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now we're ready to start preparing our dataset. For now, let's use the following set of transformations:\n",
        "\n",
        "Convert any grayscale images to RGB format with a custom class\n",
        "Resize the image, so that they're all the same size (we chose\n",
        " x\n",
        ")\n",
        "Convert the image to a Tensor of pixel values"
      ],
      "metadata": {
        "id": "_QHk7qroPuku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.2:** **Create the transformation pipeline with steps listed above by using `transforms.Compose` from the `torchvision` package**."
      ],
      "metadata": {
        "id": "QV7OQkHYP8zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "height = 224\n",
        "width = 224\n",
        "\n",
        "\n",
        "class ConvertToRGB:\n",
        "    def __call__(self, img):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        return img\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "     ConvertToRGB(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "]\n",
        ")\n",
        "\n",
        "print(transform)"
      ],
      "metadata": {
        "id": "0Msi3B7fQR_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.3: Make a dataset using ImageFolder from datasets and make sure to apply the transform transformation pipeline.**"
      ],
      "metadata": {
        "id": "hpHxcgbkP3AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "print(\"Length of dataset:\", len(dataset))"
      ],
      "metadata": {
        "id": "jVb2NM6uPsB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how small this training data is, we have just 421 images in total.\n",
        "\n",
        "** Task 2.7.4: Use this dataset to create a DataLoader with batch size 32.**"
      ],
      "metadata": {
        "id": "KP5_ULjxQheu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "batch_shape = next(iter(dataset_loader))[0].shape\n",
        "print(\"Getting batches of shape:\", batch_shape)"
      ],
      "metadata": {
        "id": "x9wnouQSQ0f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can think about normalizing our dataset, so each color channel has a mean of\n",
        " and a standard deviation of\n",
        ". In order to do this, we first need to get the mean and standard deviation of each channel in this dataset.\n",
        "\n",
        "The get_mean_std function we used in the lessons is included in the training.py file for this assignment. Let's import it."
      ],
      "metadata": {
        "id": "ev7d3Xs-Q5Ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.5: Use function get_mean_std described above and calculate the mean and standard deviation of each channel in this dataset.**"
      ],
      "metadata": {
        "id": "h16V9h1cRKT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_std(dataset_loader)\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "rM3DdwKoRGzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we can improve our original transformation pipeline. We'll take the previous one, and add normalization of the channels according to the mean and standard deviation you computed in the previous task. Our transformations should now look like:\n",
        "\n",
        "Convert any grayscale images to RGB format with a custom class\n",
        "Resize the image, so that they're all the same size (we chose\n",
        " x\n",
        ")\n",
        "Convert the image to a Tensor of pixel values\n",
        "Normalize the data using transforms.Normalize"
      ],
      "metadata": {
        "id": "jFxB-xtQRPBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.6: Build this new transformation pipeline.**"
      ],
      "metadata": {
        "id": "EC__kukBRYmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_norm = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((width, height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(transform_norm)"
      ],
      "metadata": {
        "id": "U1VXIBxxRVcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.7: Create a normalized data set, using the transformation pipeline from the previous task. Also split the normalized data set into a training set and a validation set. 80% of the data should be in the training set, and 20% in the validation set.**"
      ],
      "metadata": {
        "id": "tb0AneRrRbfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important! don't change this.\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "norm_dataset = datasets.ImageFolder(root=train_dir , transform =transform_norm)\n",
        "\n",
        "# Important, DON'T change the `generator=g` parameter\n",
        "train_dataset, val_dataset = random_split(norm_dataset,[0.8,0.2], generator=g)\n",
        "\n",
        "print(\"Length of dataset:\", len(norm_dataset))\n",
        "print(\"Training data set size:\", len(train_dataset))\n",
        "print(\"Validation data set size:\", len(val_dataset))"
      ],
      "metadata": {
        "id": "fCT9wI_ZR-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create DataLoader objects. We'll use a batch size of 32 and create one DataLoader for training and another for validation data.\n",
        "\n",
        "**Task 2.7.8: Create the training loader (with shuffling on) and the validation loader (with shuffling off).**"
      ],
      "metadata": {
        "id": "p92CPwNRSWSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important! don't change this.\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Important! Don't change the `generator=g` parameter\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, generator=g)\n",
        "val_loader = DataLoader(val_dataset,batch_size=batch_size)\n",
        "\n",
        "print(type(train_loader))\n",
        "print(type(val_loader))"
      ],
      "metadata": {
        "id": "Lti5fet7SUtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.9: Fill in the missing code below to define a neural network with the following layers:**\n",
        "\n",
        "2D convolution with sixteen\n",
        "\n",
        " kernels\n",
        "\n",
        "Max pooling with\n",
        "\n",
        " kernels (and a stride of\n",
        ")\n",
        "\n",
        "ReLU activation\n",
        "\n",
        "2D convolution with thirty-two\n",
        "\n",
        " kernels\n",
        "\n",
        "Max pooling with\n",
        "\n",
        " kernels\n",
        "\n",
        "ReLU activation\n",
        "\n",
        "2D convolution with sixty-four\n",
        "\n",
        " kernels\n",
        "\n",
        "Max pooling with\n",
        "\n",
        "\n",
        " kernels\n",
        "\n",
        "ReLU activation\n",
        "\n",
        "Flattening\n",
        "\n",
        "Drop-out\n",
        "\n",
        "Linear layer with\n",
        "\n",
        " outputs\n",
        "\n",
        "ReLU activation\n",
        "\n",
        "\n",
        "Drop-out\n",
        "\n",
        "Linear output layer with the correct number of outputs"
      ],
      "metadata": {
        "id": "z96Jug-mSg4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important! Don't change this\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model = torch.nn.Sequential()\n",
        "\n",
        "conv1_n_kernels = 16\n",
        "conv1 = torch.nn.Conv2d(\n",
        "    in_channels=3, out_channels=conv1_n_kernels, kernel_size=(3, 3), padding=1\n",
        ")\n",
        "max_pool_size = 4\n",
        "max_pool1 = torch.nn.MaxPool2d(max_pool_size)\n",
        "model.append(conv1)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool1)\n",
        "\n",
        "conv2_n_kernels = 32\n",
        "conv2 = torch.nn.Conv2d(\n",
        "    in_channels=16, out_channels=conv2_n_kernels, kernel_size=(3, 3), padding=1\n",
        ")\n",
        "max_pool2 = torch.nn.MaxPool2d(max_pool_size)\n",
        "model.append(conv2)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool2)\n",
        "\n",
        "conv3_n_kernels = 64\n",
        "conv3 = torch.nn.Conv2d(32, conv3_n_kernels, 3, padding=1)\n",
        "max_pool3 = torch.nn.MaxPool2d(max_pool_size)\n",
        "model.append(conv3)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool3)\n",
        "\n",
        "model.append(torch.nn.Flatten())\n",
        "model.append(torch.nn.Dropout())\n",
        "\n",
        "linear1 = torch.nn.Linear(in_features=576, out_features=500)\n",
        "model.append(linear1)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "\n",
        "n_classes = 3\n",
        "output_layer = torch.nn.Linear(500, n_classes)\n",
        "model.append(output_layer)\n",
        "\n",
        "summary(model, input_size=(batch_size, 3, height, width))"
      ],
      "metadata": {
        "id": "UBwhokh7SXRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.10: Define cross-entropy as the loss function and set Adam optimizer to be the optimizer. You can use the default learning settings on Adam (but it needs to know what parameters to use, so don't forget model.parameters()). Make sure to also send the model to the GPU device.**"
      ],
      "metadata": {
        "id": "mKk_RL51TbQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Make sure to send the model to the GPU\n",
        "\n",
        "print(loss_fn)\n",
        "print(\"----------------------\")\n",
        "print(optimizer)\n",
        "print(\"----------------------\")\n",
        "print(next(model.parameters()).device)"
      ],
      "metadata": {
        "id": "OX_eaYmLTasu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training the model, we can first use the functions we have in the training.py file. This model should train fairly quickly with such a small dataset, so we should leave the use_train_accuracy at default (slower training, better information).\n",
        "\n",
        "**Task 2.7.11: Use the train function from training.py to train the model for 15 epochs.**"
      ],
      "metadata": {
        "id": "uU3WyDWbTxpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the train function from `training.py`\n",
        "from training import train\n",
        "# Train the model for 15 epochs\n",
        "epochs =15\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train(model,optimizer,loss_fn, train_loader,val_loader,epochs,device=device)"
      ],
      "metadata": {
        "id": "N0TOFCBfTzHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Accuracy over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wh6C7nB-UbhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything was set up correctly, you'll notice that some learning happened. Well done! The results are not too bad, but we can do better. We could invest energy in preventing overfitting and improving the model, and training longer. But the biggest problem is that our dataset is small. A problem like this calls for Transfer Learning!\n",
        "\n",
        "**Transfer Learning**\n",
        "\n",
        "With so little data, we would be well served by getting a model that's been trained on a larger dataset. The restnet50 model we used in the lessons does well with a variety of images. Let's fetch that model and use it for Transfer Learning."
      ],
      "metadata": {
        "id": "6bE21-6uUhNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.12: Set up the resnet50 model. We'll alter it to suit our task in subsequent steps.**"
      ],
      "metadata": {
        "id": "lq4WsMsoUnaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "B3TzDkntVbde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.13: Fix the parameters of the model, so that they won't be updated when we train the model on our data.**"
      ],
      "metadata": {
        "id": "S8itbhH7Vfau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the models weights\n",
        "for params in model.parameters():\n",
        "    params.requires_grad=False\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1xkHw-pAUkhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to change the output layer of the model (layer model.fc). You'll first need to compute the number of features going into the last layer of the original model. Then you can change the last layer into:\n",
        "\n",
        "\n",
        "a dense layer with 256 neurons\n",
        "\n",
        "followed by ReLU activation\n",
        "\n",
        "then add p=0.5 of Dropout\n",
        "\n",
        "followed by the output layer with 3 outputs"
      ],
      "metadata": {
        "id": "jKGDV1zKVkln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important! Don't change this\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "in_features = model.fc.in_features\n",
        "\n",
        "modified_last_layer = nn.Sequential()\n",
        "\n",
        "dense_layer = nn.Linear(in_features,256)\n",
        "modified_last_layer.append(dense_layer)\n",
        "\n",
        "relu = nn.ReLU()\n",
        "modified_last_layer.append(relu)\n",
        "\n",
        "modified_last_layer.append(nn.Dropout(p=0.5))\n",
        "\n",
        "output_layer = nn.Linear(256,3)\n",
        "modified_last_layer.append(output_layer)\n",
        "\n",
        "# Assign `modified_last_layer` to `model.fc`\n",
        "model.fc = modified_last_layer\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "kHikeRLAWI6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important! don't change this.\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "nO6sDgvgWQBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2.7.15: Define the loss function, create an Adam optimizer and send model to GPU device. Now we are ready to train the model for\n",
        " epochs. **bold text**"
      ],
      "metadata": {
        "id": "Yidyu3CmWdp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# Place the model on device\n",
        "model.to(device)\n",
        "# Train the model for 10 epochs\n",
        "epochs = 10\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train(\n",
        "    model, optimizer, loss_fn, train_loader, val_loader, epochs, device=device\n",
        ")"
      ],
      "metadata": {
        "id": "38MAISm7WfW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Accuracy over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GkIBYDCCWhB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.16: Create a dataset for the test data. The data is located in the potato_dataset/test directory. Then make a data loader from this data set and make sure to not shuffle this data! We'll use a smaller batch size here (batch_size = 10) because the test dataset contains only 35 images.**"
      ],
      "metadata": {
        "id": "PIuNA-P4WjQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import predict\n",
        "\n",
        "test_dir =os.path.join ('potato_dataset','test')\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    root=test_dir, transform=transform_norm\n",
        ")\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Number of test images:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "6sz_de-ZYEXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to know the right category (healthy, early blight, or late blight) for each test image. We're not asked for the probabilities, so we'll need to get the actual predicted categories. Someone else looking at this won't know what our class numbers mean, so there is also code that turns the numbers into names"
      ],
      "metadata": {
        "id": "1ouu6yHjXoMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.17: Make a prediction for each of the test images.**"
      ],
      "metadata": {
        "id": "ZZQolM6CWlju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the probabilities for each test image\n",
        "test_probabilities = predict(model,test_loader , device)\n",
        "\n",
        "# Get the index associated with the largest probability for each test image\n",
        "test_predictions = torch.argmax(test_probabilities,dim=1)\n",
        "# Converts the class index to the class name for each test image.\n",
        "test_classes = [train_dataset.dataset.classes[i] for i in test_predictions]\n",
        "\n",
        "print(\"Number of predictions:\", test_predictions.shape)\n",
        "print(\"Predictions (class index):\", test_predictions.tolist())\n",
        "print()\n",
        "print(\"Predictions (class name):\", test_classes)"
      ],
      "metadata": {
        "id": "7poTQBexXkA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with Callbacks**\n",
        "\n",
        "The learning curve we obtained after training this model suggests that we've likely fit long enough. But a couple more epochs could help. The best way would be to just train for very long, but stop when training stops improving. This requires us using Callbacks.\n",
        "\n",
        "In lesson 025-callbacks, we modified the training loop to include a few callbacks. That code is in this assignment's training.py, but it's now named train_callback. Let's import it."
      ],
      "metadata": {
        "id": "MXHIa_-pYQ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_callbacks?"
      ],
      "metadata": {
        "id": "DTrkT2XOYBMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with Callbacks**\n",
        "\n",
        "The learning curve we obtained after training this model suggests that we've likely fit long enough. But a couple more epochs could help. The best way would be to just train for very long, but stop when training stops improving. This requires us using Callbacks.\n",
        "\n",
        "In lesson 025-callbacks, we modified the training loop to include a few callbacks. That code is in this assignment's training.py, but it's now named train_callback. Let's import it."
      ],
      "metadata": {
        "id": "BzxF2pSTWn1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training import train_callbacks"
      ],
      "metadata": {
        "id": "LjVeM1k8WrAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "train_callbacks?\n",
        "\n",
        "**Task 2.7.18: Continue training the model for another 50 epochs and make sure to include Early Stopping and Checkpointing. You'll need to define the checkpointing path, use \"LR_model.pth\". For early_stopping_function, there's an early_stopping function in the training.py. Import that and use it.**"
      ],
      "metadata": {
        "id": "GUxFEn5HWsPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the early_stopping function\n",
        "from training import early_stopping\n",
        "epochs_to_train = 50\n",
        "checkpoint_path = \"LR.model.pth\"\n",
        "early_stopping_function = early_stopping\n",
        "\n",
        "train_results = train_callbacks(\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=epochs_to_train,\n",
        "    device=device,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    early_stopping=early_stopping_function,\n",
        ")\n",
        "\n",
        "(\n",
        "    learning_rates,\n",
        "    train_losses,\n",
        "    valid_losses,\n",
        "    train_accuracies,\n",
        "    valid_accuracies,\n",
        "    epochs,\n",
        ") = train_results"
      ],
      "metadata": {
        "id": "Tr4IOhL3Y0cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With early stopping, the model trained a few epochs past the best model. But the checkpointing saved that best model. Let's load it."
      ],
      "metadata": {
        "id": "Ko7oBEAzZRF4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "421x6yNpZQNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.19: Load the saved best model from the checkpointing.**"
      ],
      "metadata": {
        "id": "nxFU6eNDWzjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model with `torch.load`\n",
        "checkpoint = torch.load(\"LR_model.pth\")\n",
        "# Load model state dictionaries\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "print(model)"
      ],
      "metadata": {
        "id": "FerXE_IBagr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7.20: Use the loaded best model to make predictions on each test image.**"
      ],
      "metadata": {
        "id": "r5LR8IjzW4OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the probabilities for each test image\n",
        "test_probabilities = predict(model, test_loader, device)\n",
        "\n",
        "# Get the index associated with the largest probability for each test image\n",
        "test_predictions = torch.argmax(test_probabilities, dim=1)\n",
        "\n",
        "# Converts the class index to the class name for each test image.\n",
        "test_classes = [train_dataset.dataset.classes[i] for i in test_predictions]\n",
        "\n",
        "print(\"Number of predictions:\", test_predictions.shape)\n",
        "print(\"Predictions (class index):\", test_predictions.tolist())\n",
        "print()\n",
        "print(\"Predictions (class name):\", test_classes)\n"
      ],
      "metadata": {
        "id": "G9ZEA_2YWigA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}