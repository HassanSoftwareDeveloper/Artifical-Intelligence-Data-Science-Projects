{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "4UpsW1itMolb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI Lab: Deep Learning for Computer Vision**\n"
      ],
      "metadata": {
        "id": "X3P-iQS7Mra4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Caused By Incorrect Input Layer Size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's start with imports, then define and run a PyTorch model."
      ],
      "metadata": {
        "id": "X0fAASwiMwLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J89QGGfMi93"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code crashes because of a RuntimeError, which means something went wrong when the program tried to run the model.\n",
        "\n",
        "To understand this kind of error, it's best to read the stack trace from the bottom up. The important line in your code is:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "output = model(input_tensor)\n",
        "This line sends input data through your model to get predictions. But something goes wrong during this process.\n",
        "\n",
        "The error message tells us exactly why:\n",
        "\n",
        "java\n",
        "Copy\n",
        "Edit\n",
        "mat1 and mat2 shapes cannot be multiplied (32x100 and 3200x128)\n",
        "This means you're trying to multiply two matrices, but their shapes don't line up properly:\n",
        "\n",
        "The input has shape (32, 100) → that's 32 samples with 100 features each.\n",
        "\n",
        "The model expects an input of shape (?, 3200) because its first layer is defined as nn.Linear(3200, 128).\n",
        "\n",
        "But in matrix multiplication, the number of columns in the first matrix must match the number of rows in the second matrix.\n",
        "In this case:\n",
        "\n",
        "100 columns (from your input)\n",
        "\n",
        "3200 rows (from your model layer)\n",
        "\n",
        "Those don’t match — so the multiplication fails"
      ],
      "metadata": {
        "id": "YGh8j7QHOL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "model = torch.nn.Sequential()\n",
        "linear1 = nn.Linear(in_features=100, out_features=128)\n",
        "model.append(linear1)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "linear2 = nn.Linear(in_features=128, out_features=64)\n",
        "model.append(linear2)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "linear3 = nn.Linear(in_features=64, out_features=10)\n",
        "model.append(linear3)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "print(model)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(size=(32, 100))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run the model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLrOtopuNI6p",
        "outputId": "66560103-ddb8-4276-c1ef-a1a15baf302a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=100, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Input shape: torch.Size([32, 100])\n",
            "Output shape: torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1.1: Modify the model to match the input tensor size.**"
      ],
      "metadata": {
        "id": "fAX_FbJhOPvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a revised model\n",
        "model = torch.nn.Sequential()\n",
        "linear1 = nn.Linear(in_features=10, out_features=64)\n",
        "model.append(linear1)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "linear2 = nn.Linear(in_features=64, out_features=32)\n",
        "model.append(linear2)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "linear3 = nn.Linear(in_features=32, out_features=1)\n",
        "model.append(linear3)\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(torch.nn.Dropout())\n",
        "print(model)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(size=(32, 10))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run the model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87BmdnioOSQR",
        "outputId": "e0a8ebf4-3753-4264-d6e4-22a0b9ae56e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Input shape: torch.Size([32, 10])\n",
            "Output shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Caused By Adding The Same Layer Twice\n",
        "Let's build another PyTorch model, this time a Convolutional Neural Network (CNN)."
      ],
      "metadata": {
        "id": "KdHaq8L2SpeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "model = torch.nn.Sequential()\n",
        "conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "model.append(conv1)\n",
        "model.append(torch.nn.ReLU())\n",
        "max_pool1 = nn.MaxPool2d(2, 2)\n",
        "model.append(max_pool1)\n",
        "conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "model.append(conv2)  # Add the new layer\n",
        "model.append(torch.nn.ReLU())\n",
        "model.append(max_pool1)\n",
        "print(model)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(size=(1, 3, 224, 224))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run the model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emzqz-BBSo_b",
        "outputId": "dd6cc60f-05d9-4ab9-ca3f-bb2ccae58bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Output shape: torch.Size([1, 32, 56, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the cell is executed, it also generates a RuntimeError. The message \"Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 16, 224, 224] to have 3 channels, but got 16 channels instead\" is a clue that the dimensions do not line up correctly. That is caused because the same layer was accidentally added twice to a model. This is typically caused by a copy-and-paste mistake.\n",
        "\n",
        "To resolve this issue, you'll need to carefully review your code and identify where this dimensional inconsistency occurs. Pay particular attention to the layer where you might have accidentally duplicated a component, leading to unexpected channel dimensions.\n",
        "\n",
        "The code can be fixed by adding a different layer that has the appropriate dimensions."
      ],
      "metadata": {
        "id": "XmiyWXyoStcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1.2: Fix a RuntimeError by not adding the same layer twice.**"
      ],
      "metadata": {
        "id": "09AvAUfKVKiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "model = torch.nn.Sequential()\n",
        "conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=2, padding=1)\n",
        "model.append(conv1)\n",
        "model.append(torch.nn.ReLU())\n",
        "max_pool1 = nn.MaxPool2d(2, 2)\n",
        "model.append(max_pool1)\n",
        "conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "model.append(conv2)\n",
        "model.append(torch.nn.ReLU())\n",
        "max_pool2 = nn.MaxPool2d(2, 2)\n",
        "model.append(max_pool2)\n",
        "conv3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
        "model.append(conv3)\n",
        "model.append(torch.nn.ReLU())\n",
        "max_pool3 = nn.MaxPool2d(2, 2)\n",
        "model.append(max_pool3)\n",
        "conv4 = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=2, padding=1)\n",
        "\n",
        "model.append(conv4)\n",
        "model.append(torch.nn.ReLU())\n",
        "max_pool4 = nn.MaxPool2d(2, 2)\n",
        "model.append(max_pool4)\n",
        "print(model)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(size=(1, 1, 224, 224))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run the model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEHpwjyzO4CR",
        "outputId": "650c279c-042b-48f0-f422-7a0f26138a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(1, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU()\n",
            "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (9): Conv2d(8, 1, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (10): ReLU()\n",
            "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "Input shape: torch.Size([1, 1, 224, 224])\n",
            "Output shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1.3: Fix a RuntimeError caused by forgetting to flatten.**"
      ],
      "metadata": {
        "id": "T-aYs25gVUgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model\n",
        "model = torch.nn.Sequential()\n",
        "model.append(nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "model.append(nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "model.append(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "model.append(nn.Flatten())  # add this layer for removing error\n",
        "model.append(nn.Linear(in_features=64 * 28 * 28, out_features=256))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Linear(in_features=256, out_features=64))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Linear(in_features=64, out_features=10))\n",
        "print(model)\n",
        "\n",
        "# Create a random input tensor\n",
        "input_tensor = torch.randn(size=(1, 3, 224, 224))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run the model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBYzbhYvWZAK",
        "outputId": "963e92b1-b94e-4897-9bbe-61c63d5092d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (7): ReLU()\n",
            "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (9): Flatten(start_dim=1, end_dim=-1)\n",
            "  (10): Linear(in_features=50176, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (13): ReLU()\n",
            "  (14): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Input shape: torch.Size([1, 3, 224, 224])\n",
            "Output shape: torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OAHNKUEQVVo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1.4: Fix a RuntimeError caused by incorrect dimensions after flattening.**"
      ],
      "metadata": {
        "id": "s6iYrIeuVgvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = torch.nn.Sequential()\n",
        "model.append(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "model.append(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "model.append(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "model.append(nn.Flatten())\n",
        "\n",
        "# ✅ Fixed in_features here\n",
        "model.append(nn.Linear(in_features=256 * 14 * 14, out_features=1000))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Linear(in_features=1000, out_features=10))\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Input\n",
        "input_tensor = torch.randn(size=(1, 3, 224, 224))\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "\n",
        "# Run model\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "_mJHhknBXBJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.1: Create a variable for the train directory.**"
      ],
      "metadata": {
        "id": "joZz8FRSbrBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"data_p2\"\n",
        "train_dir = os.path.join(data_dir,\"train\")\n",
        "\n",
        "print(\"Data directory:\", train_dir)"
      ],
      "metadata": {
        "id": "W_y6ECdebv2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the training directory, each class gets its own directory. There are five categories:\n",
        "\n",
        "healthy\n",
        "green mottle virus (CGM)\n",
        "bacterial blight (CBB)\n",
        "brown streak disease (CBSD)\n",
        "mosaic disease (CMD)\n",
        "We'll use the directory names as our classes."
      ],
      "metadata": {
        "id": "v_rEPxPtbx14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(train_dir)\n",
        "classes"
      ],
      "metadata": {
        "id": "Tmhf2uJtbyZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a few examples of each. The function below opens four randomly selected images with the PIL library we used in the previous project, and displays them in a line."
      ],
      "metadata": {
        "id": "MzlHYY3Wb4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(data_path, classname):\n",
        "    # Gets the files in the directory\n",
        "    class_dir = os.path.join(data_path, classname)\n",
        "    if not os.path.exists(class_dir):\n",
        "        return \"Invalid directory\"\n",
        "    image_list = os.listdir(class_dir)\n",
        "    if len(image_list) < 4:\n",
        "        return \"Not enough images in folder\"\n",
        "\n",
        "    # Pick four random images\n",
        "    images_sample = random.sample(image_list, 4)\n",
        "\n",
        "    # Plot them\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(4):\n",
        "        img_loc = os.path.join(class_dir, images_sample[i])\n",
        "        img = PIL.Image.open(img_loc)\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "YEGIxUAib1Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images(train_dir, \"cassava-healthy\")"
      ],
      "metadata": {
        "id": "eA6x8eTjb8AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.2: Use the sample_images function to look at examples of the first disease class (green mottle virus).**"
      ],
      "metadata": {
        "id": "KRDzSib1cA6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_name =  classes[0]\n",
        "print(class_name)\n",
        "\n",
        "sample_images(train_dir, class_name)"
      ],
      "metadata": {
        "id": "4xY_Qfnab-aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.3: Use the sample_images function to look at examples of the second disease class (bacterial blight).**"
      ],
      "metadata": {
        "id": "p0NNXAs5cy7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_name = classes[1]\n",
        "print(class_name)\n",
        "\n",
        "sample_images(train_dir, class_name)"
      ],
      "metadata": {
        "id": "Wk17-BdhdCTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're using the classes variable, be careful about order. We want to skip over the healthy plants, since we've already seen them."
      ],
      "metadata": {
        "id": "3AFVZG1ldDaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.4: Use the sample_images function to look at examples of the third disease class (brown streak disease).**"
      ],
      "metadata": {
        "id": "6wVlINQUdXRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_name = classes[3]\n",
        "print(class_name)\n",
        "\n",
        "sample_images(train_dir, class_name)"
      ],
      "metadata": {
        "id": "U-hLhggAdMSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.5: Use the sample_images function to look at examples of the fourth disease class (mosaic disease).**"
      ],
      "metadata": {
        "id": "6LNn6RgTdc63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_name = classes[4]\n",
        "print(class_name)\n",
        "\n",
        "sample_images(train_dir, class_name)"
      ],
      "metadata": {
        "id": "28fVDBbkfABw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Prepare our Dataset**\n",
        "Now that we've seen the images we're working with, we need to get them ready for PyTorch. We'll use the same tools as the last project to load in the data.\n",
        "\n",
        "We'll create transformations that:\n",
        "\n",
        "Convert any grayscale images to RGB format with a custom class\n",
        "Resize the image, so that they're all the same size (we chose\n",
        " x\n",
        ", but other sizes would work as well)\n",
        "Convert the image to a Tensor of pixel values"
      ],
      "metadata": {
        "id": "OpkFe1qbdfnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertToRGB():\n",
        "   def __cal__(self,img):\n",
        "      if img.mode !=\"RGB\":\n",
        "         img=img.convert(\"RGB\")\n",
        "      return img\n"
      ],
      "metadata": {
        "id": "4c7gL-6cfDKX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_basic = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "K0D1NjVrhMzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.6: Use ImageFolder to create a dataset applying the transform_basic, and DataLoader to create a data loader. Use a batch size of 32**\n"
      ],
      "metadata": {
        "id": "LoXtODp1hRi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = datasets.ImageFolder(root=train_dir,transform=transform_basic)\n",
        "dataset_loader = DataLoader(dataset,batch_size=batch_size)\n",
        "\n",
        "batch_shape = next(iter(dataset_loader))[0].shape\n",
        "print(\"Getting batches of shape:\", batch)"
      ],
      "metadata": {
        "id": "U9t04MsXiYej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize Data**"
      ],
      "metadata": {
        "id": "lZMjaO1aiiNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the last project, we saw that normalized data made our network perform better. That is data with a mean of\n",
        " and a standard deviation of\n",
        ". We'll want to normalize this data as well.\n",
        "\n",
        "This is the function we used in the last project to compute the mean and standard deviation of our images."
      ],
      "metadata": {
        "id": "5JS3_EW7ifTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bXoJGySrdWVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(loader):\n",
        "    \"\"\"Computes the mean and standard deviation of image data.\n",
        "\n",
        "    Input: a `DataLoader` producing tensors of shape [batch_size, channels, pixels_x, pixels_y]\n",
        "    Output: the mean of each channel as a tensor, the standard deviation of each channel as a tensor\n",
        "            formatted as a tuple (means[channels], std[channels])\"\"\"\n",
        "\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "    for data, _ in tqdm(loader, desc=\"Computing mean and std\", leave=False):\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "    mean = channels_sum / num_batches\n",
        "    std = (channels_squared_sum / num_batches - mean**2) ** 0.5\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "HSKtTOJFitUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.7: Run the get_mean_std function on the dataset_loader, and save the means and standard deviations to variables mean and std. There should be a value for each color channel, giving us vectors of length 3**"
      ],
      "metadata": {
        "id": "mfm_gGa_jREP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_std(dataset_loader)\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "9Gw_iVwYjQ1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As expected, the means aren't\n",
        " and the standard deviations aren't\n",
        ". We'll again change our transformations to include a Normalize. As in the last project, we need to give it a mean and standard deviation for each color channel. This is what we computed with our function."
      ],
      "metadata": {
        "id": "vEpv5CDBjhbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_norm = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "KuDJnD8_jcP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.8: Make a new normalized dataset using ImageFolder and a new DataLoader. Be sure to use a the same batch size of 32**\n",
        "."
      ],
      "metadata": {
        "id": "AlgQHXTnjlpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c-5PR0jEiulC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_dataset = datasets.ImageFolder(root=train_dir,transform=transform_norm)\n",
        "norm_loader = DataLoader(norm_dataset,batch_size=batch_size)\n",
        "\n",
        "batch_shape = next(iter(norm_loader))[0].shape\n",
        "print(\"Getting batches of shape:\", batch_shape)"
      ],
      "metadata": {
        "id": "-jrIdmrjkRWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the new normalize transformation, the data should have mean\n",
        " and standard deviation\n",
        " in each color channel. Let's check that."
      ],
      "metadata": {
        "id": "G83c7qeFkSDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.9: Use the get_mean_std function to verify the mean and standard deviation are correct in the norm_loader data.**"
      ],
      "metadata": {
        "id": "NSbd4sTLkXVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_mean, norm_std = get_mean_std(norm_loader)\n",
        "\n",
        "print(f\"Mean: {norm_mean}\")\n",
        "print(f\"Standard deviation: {norm_std}\")"
      ],
      "metadata": {
        "id": "IZZnCPWwkUfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Validation Split\n",
        "The next thing we'll need to do is create a training dataset and a validation dataset. We can use the random_split tool as we did in the last project."
      ],
      "metadata": {
        "id": "vkmJLt6wktCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 2.2.10: Use random_split to create a training dataset with 80% of the data, and a validation dataset with 20% of the data. Be sure to use norm_dataset. **bold text**"
      ],
      "metadata": {
        "id": "zJKxLdyllTkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = random_split(norm_dataset,[0.8,0.2])\n",
        "\n",
        "length_train = len(train_dataset)\n",
        "length_val = len(val_dataset)\n",
        "length_dataset = len(norm_dataset)\n",
        "percent_train = np.round(100 * length_train / length_dataset, 2)\n",
        "percent_val = np.round(100 * length_val / length_dataset, 2)\n",
        "\n",
        "print(f\"Train data is {percent_train}% of full data\")\n",
        "print(f\"Validation data is {percent_val}% of full data\")"
      ],
      "metadata": {
        "id": "bUd8IljQkqj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previous project, we want to make sure the training data and validation data are similar. We'll want to double check that the two sets have similar proportions of each class.\n",
        "\n",
        "In the previous project, we used a class_counts function to count how many observations were in each class. This function is now in the training.py file, so we can import it from there. The function takes a dataset (not a data loader) and returns a pandas Series of the counts of each class."
      ],
      "metadata": {
        "id": "QlotysdGlg57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_counts = class_counts(train_dataset)\n",
        "train_counts"
      ],
      "metadata": {
        "id": "J7ooxLHqlmpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_counts.plot(kind=\"bar\");"
      ],
      "metadata": {
        "id": "B1ep-rWBlo5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unbalanced Classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Unless you got really unlucky, the two graphs should be very similar.\n",
        "\n",
        "But the different classes have different numbers of images. There are fewer bacterial blight images, and more brown streak disease and mosaic disease images. This can cause our model to be biased — it will be less likely to call an image bacterial blight. But that doesn't reflect the world, just our training set!\n",
        "\n",
        "This is an example of unbalanced classes. We can correct this by adjusting our dataset. We can either get more images until they're about the same, or remove images from the ones with more.\n",
        "\n",
        "Since we can't get more images, we'll remove some from the larger classes. This is called undersampling. The function below will do this for us."
      ],
      "metadata": {
        "id": "04DRW_FcmgoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undersample_dataset(dataset_dir, output_dir, target_count=None):\n",
        "    \"\"\"\n",
        "    Undersample the dataset to have a uniform distribution across classes.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset_dir: Path to the directory containing the class folders.\n",
        "    - output_dir: Path to the directory where the undersampled dataset will be stored.\n",
        "    - target_count: Number of instances to keep in each class. If None, the class with the least instances will set the target.\n",
        "    \"\"\"\n",
        "    # Mapping each class to its files\n",
        "    classes_files = {}\n",
        "    for class_name in os.listdir(dataset_dir):\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            files = os.listdir(class_dir)\n",
        "            classes_files[class_name] = files\n",
        "\n",
        "    # Determine the minimum class size if target_count is not set\n",
        "    if target_count is None:\n",
        "        target_count = min(len(files) for files in classes_files.values())\n",
        "\n",
        "    # Creating the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Perform undersampling\n",
        "    for class_name, files in classes_files.items():\n",
        "        print(\"Copying images for class\", class_name)\n",
        "        class_output_dir = os.path.join(output_dir, class_name)\n",
        "        if not os.path.exists(class_output_dir):\n",
        "            os.makedirs(class_output_dir)\n",
        "\n",
        "        # Randomly select target_count images\n",
        "        selected_files = random.sample(files, min(len(files), target_count))\n",
        "\n",
        "        # Copy selected files to the output directory\n",
        "        for file_name in tqdm(selected_files):\n",
        "            src_path = os.path.join(dataset_dir, class_name, file_name)\n",
        "            dst_path = os.path.join(class_output_dir, file_name)\n",
        "            copy2(src_path, dst_path)\n",
        "\n",
        "    print(f\"Undersampling completed. Each class has up to {target_count} instances.\")"
      ],
      "metadata": {
        "id": "rLM4x6A7mhsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2.12: Create a variable output_dir for the new directory data_p2/data_undersampled/train**"
      ],
      "metadata": {
        "id": "0Fc2cVgcoiHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = os.path.join(\"data-p2\",\"data_undersampled\",\"train\")\n",
        "print(\"Output directory:\", output_dir)"
      ],
      "metadata": {
        "id": "BbnRvoSHok2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll erase that directory if it already exists, and run our function to create a new, balanced dataset."
      ],
      "metadata": {
        "id": "Q8ydzfUmon7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf {output_dir}"
      ],
      "metadata": {
        "id": "ejLyMXzvopea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersample_dataset(train_dir, output_dir)"
      ],
      "metadata": {
        "id": "ndtf92QdosHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The claim is that all classes should have the same number of images now. Let's check that by running our count. We'll need to create a new dataset first, using the new data."
      ],
      "metadata": {
        "id": "A5WqBOW5oy_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 2.2.13: Create a dataset with ImageFolder using the data in the output_dir. Transform the data with the transform_norm. **bold text**\n"
      ],
      "metadata": {
        "id": "u4lJxiDfpISb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "undersampled_dataset = datasets.ImageFolder(root=train_dir, transform=transform_norm)"
      ],
      "metadata": {
        "id": "DTN-clPhpN3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersampled_dataset.classes"
      ],
      "metadata": {
        "id": "0vkvvNPfpQSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "And all the classes should now have the same number of counts. If we recreate the bar plot, every bar should be the same height. We'll also print out the counts to make sure they're exactly the same.\n",
        "\n",
        "**Task 2.2.14: Use class_counts to make a pandas Series from the undersampled data, and create the bar chart from it.**"
      ],
      "metadata": {
        "id": "z38j_IGGpSil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important, don't change this\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "under_counts = class_counts(undersampled_dataset)\n",
        "\n",
        "# Create a bar chart from under_counts\n",
        "# important, you must leave `ax=ax`\n",
        "under_counts.plot(kind=\"bar\", ax=ax)"
      ],
      "metadata": {
        "id": "-Ol99Buhpg4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from training import class_counts"
      ],
      "metadata": {
        "id": "OIi4tZMSlgIr"
      }
    }
  ]
}